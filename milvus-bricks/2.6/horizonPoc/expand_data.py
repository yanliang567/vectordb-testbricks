import json
import os
import random
import sys
from pathlib import Path
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import h5py
import pandas as pd
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import math

# =============================
# é…ç½®å‚æ•°
# =============================
INPUT_PARQUET = "test_dataset_1M.parquet"
VECTOR_HDF5 = "cohere-768-euclidean.hdf5"
QUERY_DIR = Path("query_scenarios")

MERGE_QUERY_DIR = Path("query_scenarios_merged_vector")
MERGE_QUERY_DIR.mkdir(exist_ok=True)

OUTPUT_DIR = Path("expanded_dataset_100M")
OUTPUT_DIR.mkdir(exist_ok=True)

# æ¸…ç©ºè¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
# shutil.rmtree(OUTPUT_DIR)
# OUTPUT_DIR.mkdir()

NUM_TARGET_ROWS = 100_000_000  # 1äº¿æ¡
# NUM_TARGET_ROWS = 1_500_000  # 2Mæ¡
CHUNK_SIZE_PER_FILE = 250_000  # æ¯ä¸ªæ–‡ä»¶ ~25ä¸‡è¡Œï¼Œçº¦ 800MB~1GB
NUM_WORKERS = os.cpu_count()  # å¹¶å‘è¿›ç¨‹æ•°ï¼ˆæ¨è 4~16ï¼‰

VECTOR_DIM = 768
DTYPE_VECTOR = np.float32

# å™ªå£°å¼ºåº¦ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
NOISE_LAT_LON = 1e-5  # çº¦ Â±1 ç±³
NOISE_VECTOR = 1e-3  # å‘é‡å¾®å°æ‰°åŠ¨ï¼Œä¿æŒè¯­ä¹‰ç›¸ä¼¼


def parse_hdf5(h5_path, dataset_name='train'):
    with h5py.File(h5_path, 'r') as hf:
        if dataset_name not in hf:
            raise KeyError(f"HDF5 æ–‡ä»¶ä¸­æœªæ‰¾åˆ° {dataset_name} æ•°æ®é›†")
        vectors = hf[dataset_name]
        vector_count = vectors.shape[0]
        assert vectors.shape[1] == VECTOR_DIM, f"ç»´åº¦ä¸åŒ¹é…: {vectors.shape[1]}"
        print(f"âœ… æ•°æ®é›† {dataset_name}, å‘é‡æ•°æ®: {vector_count:,} æ¡, dim={VECTOR_DIM}")
        return vectors[:]


# =============================
# ä¸»å‡½æ•°ï¼šç”Ÿæˆä¸€æ‰¹åˆ†å—æ•°æ®
# =============================
def generate_chunk(args):
    chunk_id, start_idx, num_rows, schema, df, vectors = args

    vector_n = len(vectors)
    original_n = len(df)

    # ç”¨äºä¿å­˜ç»“æœçš„åˆ—è¡¨
    batch_data = []

    np.random.seed(chunk_id)  # ä¸åŒ chunk ç§å­ä¸åŒ

    for i in range(num_rows):
        idx = (start_idx + i) % original_n
        vec_idx = idx % vector_n  # å¾ªç¯ä½¿ç”¨å‘é‡
        row = df.iloc[idx].to_dict()
        vec = vectors[vec_idx].astype(DTYPE_VECTOR).copy()
        # æ·»åŠ å™ªå£°ï¼ˆè½»å¾®æ‰°åŠ¨ï¼‰
        if row.get('gcj02_lat') and not pd.isna(row['gcj02_lat']):
            row['gcj02_lat'] += np.random.normal(0, NOISE_LAT_LON)
            row['gcj02_lon'] += np.random.normal(0, NOISE_LAT_LON)
            row['wgs84_lat'] += np.random.normal(0, NOISE_LAT_LON)
            row['wgs84_lon'] += np.random.normal(0, NOISE_LAT_LON)
        vec += np.random.normal(0, NOISE_VECTOR, vec.shape)  # å‘é‡åŠ å™ª
        # æ·»åŠ å‘é‡å­—æ®µ
        row['vector'] = vec.tolist()  # å­˜ä¸º listï¼ŒPyArrow è‡ªåŠ¨è¯†åˆ«ä¸º list<float>
        batch_data.append(row)

    # æ„å»º Arrow è¡¨
    schema_with_vector = schema.append(
        pa.field('vector', pa.list_(pa.float32()), nullable=False)
    )

    batch_df = pd.DataFrame(batch_data)
    batch_table = pa.Table.from_pandas(batch_df, schema=schema_with_vector, safe=False)

    # å†™å‡º Parquet æ–‡ä»¶
    output_file = OUTPUT_DIR / f"part_{chunk_id:04d}.parquet"
    pq.write_table(
        batch_table,
        output_file,
        compression=None,
        use_dictionary=False,
        write_statistics=True
    )

    return len(batch_table)


# =============================
# ä¸»æµç¨‹ï¼šè°ƒåº¦å¹¶å‘ä»»åŠ¡
# =============================
def expand_to_100m():

    # è®¡ç®—æ€» chunk æ•°
    total_chunks = math.ceil(NUM_TARGET_ROWS / CHUNK_SIZE_PER_FILE)
    chunks = []

    for chunk_id in range(total_chunks):
        start_idx = chunk_id * CHUNK_SIZE_PER_FILE
        num_rows = min(CHUNK_SIZE_PER_FILE, NUM_TARGET_ROWS - start_idx)
        chunks.append((chunk_id, start_idx, num_rows, table.schema, df_original, train_vectors))

    print(f"ğŸš€ å¼€å§‹å¹¶å‘ç”Ÿæˆ {total_chunks} ä¸ªæ–‡ä»¶ï¼Œç›®æ ‡æ€»é‡: {NUM_TARGET_ROWS:,}")
    print(f"âš™ï¸  ä½¿ç”¨ {NUM_WORKERS} ä¸ªè¿›ç¨‹ï¼Œå¹¶å‘å†™å…¥...")

    completed = 0
    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:
        futures = [executor.submit(generate_chunk, arg) for arg in chunks]

        # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ•´ä½“è¿›åº¦
        with tqdm(total=len(futures), desc="ğŸ“¦ ç”Ÿæˆæ–‡ä»¶", unit="file") as pbar:
            for future in as_completed(futures):
                try:
                    count = future.result()
                    completed += count
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")
                finally:
                    pbar.update(1)
                    pbar.set_postfix({"ç´¯è®¡è¡Œæ•°": f"{completed:,}"})

    print(f"\nğŸ‰ å®Œæˆï¼å·²ç”Ÿæˆ {completed:,} æ¡æ•°æ®ï¼Œå­˜å‚¨äº {OUTPUT_DIR}/")


def merge_queries(test_vectors):
    query_files = [QUERY_DIR / f for f in os.listdir(QUERY_DIR) if f.endswith(".jsonl")]
    if not query_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æŸ¥è¯¢æ–‡ä»¶ï¼")
        return
    # å¤–å±‚ï¼šéå†æ–‡ä»¶
    for query_file in query_files:
        with open(query_file, 'r', encoding='utf-8') as f:
            queries = [json.loads(line.strip()) for line in f if line.strip()]
            query_vector_path = MERGE_QUERY_DIR / f"{query_file.stem}_vector.jsonl"
            query_double_vector_path = MERGE_QUERY_DIR / f"{query_file.stem}_double_vector.jsonl"
            with open(query_vector_path, 'w', encoding='utf-8') as fout, \
                 open(query_double_vector_path, 'w', encoding='utf-8') as fout2:
                for orig_q in queries:
                    record = orig_q.copy()
                    record['pic_embedding'] = random.choice(test_vectors).tolist()
                    fout.write(json.dumps(record, ensure_ascii=False) + "\n")
                    record['text_embedding'] = random.choice(test_vectors).tolist()
                    fout2.write(json.dumps(record, ensure_ascii=False) + "\n")


# =============================
# è¿è¡Œ
# =============================
if __name__ == "__main__":
    if sys.argv[1] not in ['train', 'test']:
        print("âŒ å‚æ•°é”™è¯¯ï¼Œè¯·è¾“å…¥ 'train' æˆ– 'test'")
        sys.exit(1)

    print("ğŸ” æ­£åœ¨è¯»å–æºæ•°æ®...")
    # è¯»å–ä¸€æ¬¡è·å– schema
    table = pq.read_table(INPUT_PARQUET)
    print(f"âœ… æ ‡é‡åŸå§‹æ•°æ®: {table.num_rows:,} è¡Œ")
    df_original: pd.DataFrame = table.to_pandas()

    if sys.argv[1] == 'train':
        train_vectors = parse_hdf5(VECTOR_HDF5, 'train')
        expand_to_100m()
    elif sys.argv[1] == 'test':
        test_vectors = parse_hdf5(VECTOR_HDF5, 'test')
        merge_queries(test_vectors)


