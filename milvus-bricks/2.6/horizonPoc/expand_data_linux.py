import json
import os
import random
import sys
from pathlib import Path
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
import h5py
import pandas as pd
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import math

# =============================
# é…ç½®å‚æ•°
# =============================
INPUT_PARQUET = "data/test_dataset_1M.parquet"
VECTOR_HDF5 = "data/cohere-768-euclidean.hdf5"
QUERY_DIR = Path("data/query_scenarios")

MERGE_QUERY_DIR = Path("data/query_scenarios_merged_vector")
MERGE_QUERY_DIR.mkdir(exist_ok=True)

OUTPUT_DIR = Path("data/expanded_dataset_100M")
OUTPUT_DIR.mkdir(exist_ok=True)

NUM_TARGET_ROWS = 100_000_000
# NUM_TARGET_ROWS = 1_500_000  # æµ‹è¯•ç”¨ 150ä¸‡æ¡
CHUNK_SIZE_PER_FILE = 250_000
NUM_WORKERS = min(os.cpu_count(), 8)  # æ§åˆ¶å¹¶å‘æ•°é¿å…èµ„æºè€—å°½

VECTOR_DIM = 768
DTYPE_VECTOR = np.float32

NOISE_LAT_LON = 1e-5
NOISE_VECTOR = 1e-3

# ---------------------------------
# å…¨å±€å˜é‡ï¼ˆç”±ä¸»è¿›ç¨‹åˆå§‹åŒ–ï¼‰
# å­è¿›ç¨‹ä¼šåœ¨ generate_chunk ä¸­é‡æ–°åŠ è½½ï¼ˆæ¨¡æ‹Ÿâ€œå…±äº«â€ï¼‰
# ---------------------------------
df_original = None
train_vectors = None
test_vectors = None
table_schema = None


def parse_hdf5(h5_path, dataset_name='train'):
    with h5py.File(h5_path, 'r') as hf:
        if dataset_name not in hf:
            raise KeyError(f"HDF5 æ–‡ä»¶ä¸­æœªæ‰¾åˆ° {dataset_name} æ•°æ®é›†")
        vectors = hf[dataset_name][:]
        vector_count = vectors.shape[0]
        assert vectors.shape[1] == VECTOR_DIM, f"ç»´åº¦ä¸åŒ¹é…: {vectors.shape[1]}"
        print(f"âœ… æ•°æ®é›† {dataset_name}, å‘é‡æ•°æ®: {vector_count:,} æ¡, dim={VECTOR_DIM}")
        return vectors.astype(DTYPE_VECTOR)


# =============================
# ä¸»å‡½æ•°ï¼šç”Ÿæˆä¸€æ‰¹åˆ†å—æ•°æ®ï¼ˆå­è¿›ç¨‹è°ƒç”¨ï¼‰
# =============================
def generate_chunk(args):
    global df_original, train_vectors  # ç¡®ä¿å·²åŠ è½½

    chunk_id, start_idx, num_rows = args

    # âš ï¸ å­è¿›ç¨‹å¿…é¡»è‡ªå·±åŠ è½½æ•°æ®ï¼ˆæˆ–ç»§æ‰¿åªè¯»å‰¯æœ¬ï¼‰
    # ä½†æˆ‘ä»¬å‡è®¾ä¸»è¿›ç¨‹å·²è®¾ç½®å¥½å…¨å±€å˜é‡
    if df_original is None or train_vectors is None:
        raise RuntimeError("å­è¿›ç¨‹ä¸­æœªæ­£ç¡®åŠ è½½ df_original æˆ– train_vectors")

    original_n = len(df_original)
    vector_n = len(train_vectors)

    batch_data = []
    np.random.seed(chunk_id)

    for i in range(num_rows):
        idx = (start_idx + i) % original_n
        vec_idx = idx % vector_n

        row = df_original.iloc[idx].to_dict()
        vec = train_vectors[vec_idx].copy()

        # åæ ‡åŠ å™ª
        if pd.notna(row.get('gcj02_lat')):
            row['gcj02_lat'] += np.random.normal(0, NOISE_LAT_LON)
            row['gcj02_lon'] += np.random.normal(0, NOISE_LAT_LON)
            row['wgs84_lat'] += np.random.normal(0, NOISE_LAT_LON)
            row['wgs84_lon'] += np.random.normal(0, NOISE_LAT_LON)

        # å‘é‡åŠ å™ª
        vec += np.random.normal(0, NOISE_VECTOR, vec.shape)

        row['vector'] = vec.tolist()
        batch_data.append(row)

    schema_with_vector = pa.schema([
        *table_schema,
        pa.field('vector', pa.list_(pa.float32()), nullable=False)
    ])
    batch_table = pa.Table.from_pandas(pd.DataFrame(batch_data), schema=schema_with_vector)

    output_file = OUTPUT_DIR / f"part_{chunk_id:04d}.parquet"
    pq.write_table(batch_table, output_file, compression=None, use_dictionary=False)
    return len(batch_table)


# =============================
# åˆå§‹åŒ–å‡½æ•°ï¼ˆä¾›ä¸»è¿›ç¨‹è°ƒç”¨ï¼‰
# =============================
def init_worker():
    """å¯é€‰ï¼šç”¨äºåˆå§‹åŒ–æ—¥å¿—ã€éšæœºç§å­ç­‰"""
    pass


# =============================
# ä¸»æµç¨‹ï¼šè°ƒåº¦å¹¶å‘ä»»åŠ¡
# =============================
def expand_to_100m():
    global df_original, train_vectors, table_schema

    print("ğŸ§  æ­£åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®åˆ°å†…å­˜...")
    table = pq.read_table(INPUT_PARQUET)
    df_original = table.to_pandas()
    train_vectors = parse_hdf5(VECTOR_HDF5, 'train')
    table_schema = table.schema

    total_chunks = math.ceil(NUM_TARGET_ROWS / CHUNK_SIZE_PER_FILE)
    chunks = []

    for chunk_id in range(total_chunks):
        start_idx = chunk_id * CHUNK_SIZE_PER_FILE
        num_rows = min(CHUNK_SIZE_PER_FILE, NUM_TARGET_ROWS - start_idx)
        chunks.append((chunk_id, start_idx, num_rows))

    print(f"ğŸš€ å¼€å§‹å¹¶å‘ç”Ÿæˆ {total_chunks} ä¸ªæ–‡ä»¶ï¼Œç›®æ ‡æ€»é‡: {NUM_TARGET_ROWS:,}")
    print(f"âš™ï¸  ä½¿ç”¨ {NUM_WORKERS} ä¸ªè¿›ç¨‹ï¼Œå¹¶å‘å†™å…¥...")

    completed = 0
    with ProcessPoolExecutor(
        max_workers=NUM_WORKERS,
        initializer=init_worker
    ) as executor:
        futures = [executor.submit(generate_chunk, arg) for arg in chunks]

        with tqdm(total=len(futures), desc="ğŸ“¦ ç”Ÿæˆæ–‡ä»¶", unit="file") as pbar:
            for future in as_completed(futures):
                try:
                    count = future.result()
                    completed += count
                except Exception as e:
                    print(f"âŒ ä»»åŠ¡å¤±è´¥: {type(e).__name__}: {e}")
                finally:
                    pbar.update(1)
                    pbar.set_postfix({"ç´¯è®¡è¡Œæ•°": f"{completed:,}"})

    print(f"\nğŸ‰ å®Œæˆï¼å·²ç”Ÿæˆ {completed:,} æ¡æ•°æ®ï¼Œå­˜å‚¨äº {OUTPUT_DIR}/")


def merge_queries(test_vectors_arg, target_query_num: int = 10000):
    query_files = [f for f in QUERY_DIR.iterdir() if f.suffix == '.jsonl']
    if not query_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•æŸ¥è¯¢æ–‡ä»¶ï¼")
        return

    vectors_count = len(test_vectors_arg)
    for query_file in query_files:
        with open(query_file, 'r', encoding='utf-8') as f:
            queries = [json.loads(line.strip()) for line in f if line.strip()]

        # æ·»åŠ  pic_embedding
        with open(MERGE_QUERY_DIR / f"{query_file.stem}_vector.jsonl", 'w', encoding='utf-8') as fout:
            for idx in tqdm(range(target_query_num), desc=f"ğŸ“ Generating {query_file.stem}_vector.jsonl", unit="query"):
                q_out = queries[idx % len(queries)].copy()
                vec = test_vectors_arg[idx % vectors_count].copy()
                vec += np.random.normal(0, NOISE_VECTOR, vec.shape)
                q_out['pic_embedding'] = vec.tolist()
                fout.write(json.dumps(q_out, ensure_ascii=False) + "\n")

        # æ·»åŠ  pic_embedding + text_embedding
        with open(MERGE_QUERY_DIR / f"{query_file.stem}_double_vector.jsonl", 'w', encoding='utf-8') as fout:
            for idx in tqdm(range(target_query_num), desc=f"ğŸ“ Generating {query_file.stem}_double_vector.jsonl", unit="query"):
                q_out = queries[idx % len(queries)].copy()
                vec = test_vectors_arg[idx % vectors_count].copy()
                q_out['pic_embedding'] = vec.tolist()
                vec += np.random.normal(0, NOISE_VECTOR, vec.shape)
                q_out['text_embedding'] = vec.tolist()
                fout.write(json.dumps(q_out, ensure_ascii=False) + "\n")

        print(f"âœ… å·²å¤„ç†: {query_file.name}")


# =============================
# è¿è¡Œ
# =============================
if __name__ == "__main__":
    if len(sys.argv) < 2 or sys.argv[1] not in ['train', 'test']:
        print("âŒ ç”¨æ³•: python script.py [train|test]")
        sys.exit(1)

    mode = sys.argv[1]

    if mode == 'train':
        expand_to_100m()
    elif mode == 'test':
        print("ğŸ” åŠ è½½æµ‹è¯•å‘é‡ç”¨äºåˆå¹¶æŸ¥è¯¢...")
        test_vecs = parse_hdf5(VECTOR_HDF5, 'test')
        merge_queries(test_vecs)
