#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå¹¶å‘æŸ¥è¯¢æµ‹è¯• - ç§»é™¤å®¢æˆ·ç«¯è¿æ¥æ± 

å…³é”®ç®€åŒ–:
1. ç§»é™¤ MilvusClientPoolï¼Œä½¿ç”¨å•ä¸ª MilvusClient å®ä¾‹
2. ä¾èµ– Milvus æœåŠ¡ç«¯çš„è¿æ¥å¤ç”¨æœºåˆ¶
3. å‡å°‘ä»£ç å¤æ‚åº¦å’Œå†…å­˜å¼€é”€
4. ä¿æŒå¹¶å‘æŸ¥è¯¢èƒ½åŠ›
"""

import time
import sys
import random
import numpy as np
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from collections import deque
from pymilvus import MilvusClient, DataType

LOG_FORMAT = "%(asctime)s - %(levelname)s - %(message)s"
DATE_FORMAT = "%m/%d/%Y %H:%M:%S %p"


class OptimizedStats:
    """ä¼˜åŒ–çš„ç»Ÿè®¡ç³»ç»Ÿ"""
    
    def __init__(self, max_samples=1000):
        self.latencies = deque(maxlen=max_samples)
        self.total_queries = 0
        self.total_failures = 0
        self.start_time = time.time()
        self.lock = Lock()
    
    def record_query(self, latency, success=True):
        """è®°å½•æŸ¥è¯¢ç»“æœ"""
        with self.lock:
            self.latencies.append(latency)
            self.total_queries += 1
            if not success:
                self.total_failures += 1
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            if not self.latencies:
                return {
                    'total_queries': 0,
                    'failures': 0,
                    'qps': 0,
                    'avg_latency': 0,
                    'p99_latency': 0
                }
            
            elapsed_time = time.time() - self.start_time
            latency_array = np.array(self.latencies)
            
            return {
                'total_queries': self.total_queries,
                'failures': self.total_failures,
                'success_rate': (self.total_queries - self.total_failures) / max(self.total_queries, 1) * 100,
                'qps': self.total_queries / max(elapsed_time, 0.001),
                'avg_latency': float(np.mean(latency_array)),
                'p95_latency': float(np.percentile(latency_array, 95)),
                'p99_latency': float(np.percentile(latency_array, 99)),
                'min_latency': float(np.min(latency_array)),
                'max_latency': float(np.max(latency_array))
            }
    
    def reset_samples(self):
        """é‡ç½®æ ·æœ¬æ•°æ®ï¼ˆä¿ç•™æ€»è®¡æ•°ï¼‰"""
        with self.lock:
            self.latencies.clear()


def generate_random_expression(base_expr):
    """ç”ŸæˆéšæœºæŸ¥è¯¢è¡¨è¾¾å¼"""
    keywords = ["con%", "%nt", "%con%", "%content%", "%co%nt", "%con_ent%", "%co%nt%"]
    keyword = random.choice(keywords)
    return f'content like "{keyword}"'


def single_query_task(client, collection_name, base_expr, output_fields, limit, timeout=60):
    """
    å•ä¸ªæŸ¥è¯¢ä»»åŠ¡ - ç›´æ¥ä½¿ç”¨å…±äº«çš„ MilvusClient
    
    æ³¨æ„: ä¾èµ– MilvusClient çš„çº¿ç¨‹å®‰å…¨æ€§å’Œ Milvus æœåŠ¡ç«¯è¿æ¥å¤ç”¨
    """
    start_time = time.time()
    
    try:
        current_expr = generate_random_expression(base_expr)
        
        # ç›´æ¥ä½¿ç”¨å…±äº«çš„å®¢æˆ·ç«¯å®ä¾‹
        result = client.query(
            collection_name=collection_name,
            filter=current_expr,
            output_fields=output_fields,
            limit=limit,
            timeout=timeout
        )
        
        latency = time.time() - start_time
        return {
            'success': True,
            'latency': latency,
            'result_count': len(result) if result else 0,
            'expression': current_expr
        }
    
    except Exception as e:
        latency = time.time() - start_time
        return {
            'success': False,
            'latency': latency,
            'error': str(e),
            'expression': current_expr
        }


def execute_concurrent_batch(client, collection_name, expr, output_fields, 
                           batch_size, batch_concurrency, limit):
    """
    æ‰§è¡Œå¹¶å‘æ‰¹æ¬¡ - ä½¿ç”¨å…±äº«å®¢æˆ·ç«¯
    """
    with ThreadPoolExecutor(max_workers=batch_concurrency, 
                           thread_name_prefix=f"BatchWorker") as batch_executor:
        
        # æäº¤æ‰¹æ¬¡å†…çš„æ‰€æœ‰ä»»åŠ¡
        futures = []
        for _ in range(batch_size):
            # æ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ª client å®ä¾‹
            future = batch_executor.submit(
                single_query_task,
                client, collection_name, expr, output_fields, limit
            )
            futures.append(future)
        
        # æ”¶é›†ç»“æœ
        batch_results = []
        for future in as_completed(futures, timeout=60):
            try:
                result = future.result(timeout=60.0)
                batch_results.append(result)
            except Exception as e:
                logging.warning(f"Batch task failed: {e}")
                batch_results.append({
                    'success': False,
                    'latency': 0.1,
                    'error': str(e)
                })
        
        return batch_results


def query_permanently_simplified(client, collection_name, max_workers, 
                                output_fields, expr, timeout, batch_size=100, 
                                batch_concurrency=None, limit=None):
    """
    ç®€åŒ–ç‰ˆæœ¬çš„æŒç»­æŸ¥è¯¢æµ‹è¯• - æ— è¿æ¥æ± 
    
    :param client: å•ä¸ªå…±äº«çš„ MilvusClient å®ä¾‹
    """
    stats = OptimizedStats()
    end_time = time.time() + timeout
    total_batches = 0
    
    # å¦‚æœæœªæŒ‡å®šæ‰¹æ¬¡å¹¶å‘æ•°ï¼Œé»˜è®¤ç­‰äºbatch_size
    if batch_concurrency is None:
        batch_concurrency = min(batch_size, 20)  # åˆç†é™åˆ¶ï¼Œé¿å…è¿‡å¤šçº¿ç¨‹
    
    logging.info(f"Starting simplified query test:")
    logging.info(f"  Max Workers: {max_workers} (æ‰¹æ¬¡æ§åˆ¶)")
    logging.info(f"  Batch Size: {batch_size} (æ¯æ‰¹æ¬¡ä»»åŠ¡æ•°)")
    logging.info(f"  Batch Concurrency: {batch_concurrency} (æ‰¹æ¬¡å†…éƒ¨å¹¶å‘)")
    logging.info(f"  Client: Shared single MilvusClient instance")
    
    # ä¸»æ§åˆ¶å¾ªç¯
    with ThreadPoolExecutor(max_workers=max_workers, 
                           thread_name_prefix="BatchController") as main_executor:
        
        while time.time() < end_time:
            if time.time() >= end_time:
                logging.info("Timeout reached, exiting main loop")
                break
            
            batch_start_time = time.time()
            remaining_time = end_time - time.time()
            
            if remaining_time <= 0:
                break
            
            # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
            current_batch_size = min(batch_size, max(1, int(remaining_time * 10)))
            
            # æäº¤æ‰¹æ¬¡æ‰§è¡Œä»»åŠ¡
            batch_future = main_executor.submit(
                execute_concurrent_batch,
                client, collection_name, expr, output_fields,
                current_batch_size, batch_concurrency, limit
            )
            
            try:
                # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
                batch_results = batch_future.result(timeout=min(remaining_time, 60))
                
                # è®°å½•ç»Ÿè®¡
                for result in batch_results:
                    stats.record_query(result['latency'], result['success'])
                
                total_batches += 1
                batch_duration = time.time() - batch_start_time
                
                # å®šæœŸè¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                logging_batch = 10
                if total_batches % logging_batch == 0:
                    current_stats = stats.get_stats()
                    logging.info(
                        f"Batch {total_batches}: {len(batch_results)} queries in {batch_duration:.2f}s, "
                        f"QPS: {current_stats['qps']:.1f}, "
                        f"Avg: {current_stats['avg_latency']:.3f}s, "
                        f"P99: {current_stats['p99_latency']:.3f}s, "
                        f"Success Rate: {current_stats['success_rate']:.1f}%, "
                        f"Total: {current_stats['total_queries']}"
                    )
                    
                    # é‡ç½®æ ·æœ¬æ•°æ®ä»¥é¿å…å†…å­˜æ— é™å¢é•¿
                    if total_batches % (logging_batch * 100) == 0:
                        stats.reset_samples()
                        
            except Exception as e:
                logging.warning(f"Batch execution failed: {e}")
                break
    
    # æœ€ç»ˆç»Ÿè®¡
    final_stats = stats.get_stats()
    logging.info("=" * 80)
    logging.info("FINAL PERFORMANCE STATISTICS:")
    logging.info(f"  Total Queries: {final_stats['total_queries']}")
    logging.info(f"  Total Failures: {final_stats['failures']}")
    logging.info(f"  Success Rate: {final_stats['success_rate']:.2f}%")
    logging.info(f"  Overall QPS: {final_stats['qps']:.2f}")
    logging.info(f"  Average Latency: {final_stats['avg_latency']:.3f}s")
    logging.info(f"  P95 Latency: {final_stats['p95_latency']:.3f}s")
    logging.info(f"  P99 Latency: {final_stats['p99_latency']:.3f}s")
    logging.info("=" * 80)
    
    return final_stats


def verify_collection_setup(client, collection_name):
    """éªŒè¯é›†åˆè®¾ç½®"""
    if not client.has_collection(collection_name=collection_name):
        logging.error(f"Collection {collection_name} does not exist")
        return False
            
    # æ£€æŸ¥é›†åˆæ˜¯å¦å·²åŠ è½½
    load_state = client.get_load_state(collection_name=collection_name)
    if load_state.get('state') != 'Loaded':
        logging.info(f"Loading collection {collection_name}...")
        client.load_collection(collection_name=collection_name)
        logging.info(f"Collection {collection_name} loaded successfully")
       
    return True


if __name__ == '__main__':
    if len(sys.argv) not in [11]:
        print("Usage: python3 query_permanently_simplified.py <host> <collection> <max_workers> <timeout> <output_fields> <expression> <api_key> <batch_size> [batch_concurrency]")
        print("Parameters:")
        print("  host             : Milvus server host")
        print("  collection       : Collection name")
        print("  max_workers      : Maximum concurrent batches")
        print("  timeout          : Test timeout in seconds")
        print("  output_fields    : Fields to return (comma-separated or '*')")
        print("  expression       : Query filter expression")
        print("  limit            : Query limit")
        print("  api_key          : API key (or 'None' for local)")
        print("  batch_size       : Number of queries per batch")
        print("  batch_concurrency: Concurrent threads within each batch (optional)")
        print()
        print("Examples:")
        print("  # åŸºç¡€ä½¿ç”¨ - å•å®¢æˆ·ç«¯ï¼Œæ— è¿æ¥æ± ")
        print("  python3 query_permanently_simplified.py localhost test_collection 1 60 'id' 'id>0' None 50 5")
        print()
        print("  # æ‰¹æ¬¡å†…å¹¶å‘")
        print("  python3 query_permanently_simplified.py localhost test_collection 1 60 'id' 'id>0' None 50 10 5")
        print()
        print("  # é«˜æ€§èƒ½é…ç½®")
        print("  python3 query_permanently_simplified.py localhost test_collection 2 60 'id' 'id>0' None 50 20 10")
        print()
        print("ğŸ”§ å…³é”®æ”¹è¿›:")
        print("  âœ… ç§»é™¤äº†å®¢æˆ·ç«¯è¿æ¥æ± ")
        print("  âœ… ä½¿ç”¨å•ä¸ªå…±äº«çš„ MilvusClient å®ä¾‹")
        print("  âœ… ä¾èµ– Milvus æœåŠ¡ç«¯è¿æ¥å¤ç”¨")
        print("  âœ… æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨å’Œä»£ç å¤æ‚åº¦")
        sys.exit(1)
    
    host = sys.argv[1]
    name = str(sys.argv[2])
    max_workers = int(sys.argv[3])
    timeout = int(sys.argv[4])
    output_fields = str(sys.argv[5]).strip()
    expr = str(sys.argv[6]).strip()
    limit = int(sys.argv[7])
    api_key = str(sys.argv[8])
    batch_size = int(sys.argv[9])
    batch_concurrency = int(sys.argv[10])

    port = 19530
    
    # å‚æ•°å¤„ç†
    if timeout <= 0:
        timeout = 2 * 3600
    
    if output_fields in ["None", "none", "NONE"] or output_fields == "":
        output_fields = ["*"]
    else:
        output_fields = output_fields.split(",")
    
    if expr in ["None", "none", "NONE"] or expr == "":
        expr = "None"
    if limit in [None, "None", "none", "NONE"] or limit == "":
        limit = None
        
    # è®¾ç½®æ—¥å¿—
    log_filename = f"/tmp/query_simplified_{name}_{int(time.time())}.log"
    file_handler = logging.FileHandler(filename=log_filename)
    stdout_handler = logging.StreamHandler(stream=sys.stdout)
    handlers = [file_handler, stdout_handler]
    logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, datefmt=DATE_FORMAT, handlers=handlers)
    
    logging.info("ğŸš€ Starting SIMPLIFIED query_permanently test:")
    logging.info(f"  Host: {host}")
    logging.info(f"  Collection: {name}")
    logging.info(f"  Max Workers: {max_workers}")
    logging.info(f"  Timeout: {timeout}s")
    logging.info(f"  Output Fields: {output_fields}")
    logging.info(f"  Expression: {expr}")
    logging.info(f"  Limit: {limit}")
    logging.info(f"  Batch Size: {batch_size}")
    logging.info(f"  Batch Concurrency: {batch_concurrency or 'auto'}")

    # åˆ›å»ºå•ä¸ªå…±äº«å®¢æˆ·ç«¯ - å…³é”®ç®€åŒ–ï¼
    try:
        if api_key is None or api_key == "" or api_key.upper() == "NONE":
            client = MilvusClient(uri=f"http://{host}:{port}")
        else:
            client = MilvusClient(uri=host, token=api_key)
        
        logging.info(f"âœ… Created single shared MilvusClient for {host}")
        
        # éªŒè¯é›†åˆ
        if not verify_collection_setup(client, name):
            logging.error(f"Collection '{name}' setup verification failed")
            sys.exit(1)
        
    except Exception as e:
        logging.error(f"Failed to create MilvusClient: {e}")
        sys.exit(1)
    
    # è¿è¡Œç®€åŒ–çš„æŸ¥è¯¢æµ‹è¯•
    try:
        start_time = time.time()
        final_stats = query_permanently_simplified(
            client=client,  # ä¼ é€’å•ä¸ªå®¢æˆ·ç«¯è€Œä¸æ˜¯è¿æ¥æ± 
            collection_name=name,
            max_workers=max_workers,
            output_fields=output_fields,
            expr=expr,
            timeout=timeout,
            batch_size=batch_size,
            batch_concurrency=batch_concurrency,
            limit=limit
        )
        end_time = time.time()
        
        logging.info(f"âœ… Simplified query test completed in {end_time - start_time:.2f} seconds")
        logging.info(f"ğŸ“Š Final QPS: {final_stats['qps']:.2f}")
        logging.info(f"ğŸ“ Log file: {log_filename}")
        
    except KeyboardInterrupt:
        logging.info("âš ï¸ Query test interrupted by user")
    except Exception as e:
        logging.error(f"âŒ Query test failed: {e}")
        raise
    finally:
        # ç®€åŒ–çš„æ¸…ç†ï¼šåªéœ€è¦å…³é—­ä¸€ä¸ªå®¢æˆ·ç«¯
        try:
            if hasattr(client, 'close'):
                client.close()
            logging.info("ğŸ”Œ MilvusClient closed")
        except Exception as e:
            logging.warning(f"Failed to close client: {e}")
